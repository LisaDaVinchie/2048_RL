{
    "agent":{
        "action_size": 4,
        "grid_size": 4,
        "n_channels": 11,
        "gamma": 0.995,
        "epsilon": 1,
        "epsilon_decay": 0.995,
        "epsilon_min": 0.01,
        "buffer_maxlen": 100000,
        "batch_size": 1024,
        "target_update_freq": 50,
        "learn_iterations": 10,
        "explore_for": 10,
        "steps_ahead": 2,
        "epsilon_decay_kind": "multiply",
        "representation_kind": "log2",
        "_possible_decay_kinds": ["multiply", "exponential"],
        "_possible_representation_kinds": ["raw", "log2", "one_hot"]
    },
    "training": {
        "n_episodes": 10,
        "learning_rate": 0.0001,
        "print_every": 10,
        "model_kind": "linear",
        "optimizer_kind": "adam",
        "loss_kind": "mse",
        "_possible_models": ["cnn", "linear"],
        "_possible_losses": ["mse", "huber"],
        "_possible_optimizers": ["adam", "sgd"]
    },
    "CNN_model": {
        "middle_channels": [64, 64, 64, 128],
        "kernel_sizes": [3, 3, 3],
        "padding": [1, 1, 1],
        "softmax": false
    },
    "Linear_model": {
        "middle_channels": [1024, 1024, 1024]
    },
    "actions": {
        "up": 0,
        "down": 1,
        "left": 2,
        "right": 3
    },
    "rewards": {
        "max_tile_reward": 100,
        "empty_cells_reward": 5,
        "game_over_penalty": 1000,
        "no_changes_penalty": 5
    }
}